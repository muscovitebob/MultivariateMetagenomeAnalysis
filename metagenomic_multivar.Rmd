---
title: "Multivariate statistical analysis of metagenomic data"
author: "Boris Shilov"
bibliography: "bibliography.bib"
output:
  pdf_document: default
---

# Introduction

The aim of this analysis is to explore the functional space of the metagenomic dataset assembled by @Forslund:2015aa. My primary objective here is to determine, using dimensionality reduction and ordination methods, whether there is some structure in the data, and to see whether this structure is in any way associated with the treatment/disease status of the samples, and to do this without taking taxonomic information into account (hence the limitation to functional space).

## Data format

```{r warning=FALSE, message=FALSE}
library(tidyverse)
set.seed(100)
data = read_tsv("data.r")
```

The dataset consists of six columns. The first is sample IDs, which are not unique keys since every sample may have multiple features. The second is the dataset of origin, which is either Danish (MHD), Swedish (SWE) or Han Chinese (CHN). The third column is treatment status of a particular patient, where they are either healthy (ND CTRL), are having type 2 diabetes treated with metformin included (T2D metformin+), are having type 2 diabetes treated without metformin (T2D metformin-) or are having type 1 diabetes treated (T1D). The fourth column specifies the type of feature in the fifth column. This is either gut microbial/metabolic module (GMM), SEED database annotation, bacterial family, bacterial genus or metagenomic operational taxonomic units (Motu). The fifth column contains the feature names which are coded differently depending on the feature type. Finally, the sixth column contains the abundance of a given feature.

The feature types are split into two spaces: the taxonomic space represented by the genuses, families and taxonomic units, and the functional space as annotated by SEED and GMM. The full SEED and GMM feature code annotations can be found in the appendix files, taken from Supplementary Table 10 of @Forslund:2015aa. We are only interested in exploring the structure of the functional space.

```{r}
functional_space = filter(data, FeatureType %in% c("GMM", "SEED"))
functional_space
```

In order to actually perform analysis, we transform the data into the data matrix $X$, with every row being a sample and every column a particular feature. First we combine redundant columns. Some ecological statistical functions will assume the transpose of this format, so we will stick to using a data frame after our initial processing here - tibbles are not transposed easily due to not implementng row names.

```{r}
united_functional_space = functional_space %>%
  unite(SampleInfo, Sample, Dataset, Status, sep="~") %>% 
  unite(FeatureInfo, FeatureType, Feature, sep="~")
X = united_functional_space %>% group_by(FeatureInfo) %>% spread(FeatureInfo, Abundance)
```

There is significant missingness in 25 of the individuals, and we exclude them from further analysis.

```{r}
X = drop_na(X)
separated_X = X %>% separate(SampleInfo, into=c("Sample", "Dataset", "Status"), sep="~")
X.df = X %>% column_to_rownames("SampleInfo") %>% as.data.frame()
```

```{r}
sum(X==0)/(dim(X)[1] * dim(X)[2])
```

30% of our matrix is sparse.

# Analysis

## Log transformation of the data

As we have an abudance matrix, we need to transform the data to be linear. We can do this using a log transformation, in particular the centered log ratio transform. In order to log transform, we first need to get rid of the zeroes, which we use Bayesian multiplicative replacement for. Normally we would have to normalise and filter the data for low-abundance samples, but this has already been done for us.

```{r}
library(zCompositions)
X.df.czm = cmultRepl(X.df,  label=0, method="CZM")
X.df.clr = t(apply(X.df.czm, 1, function(x) {log(x) - mean(log(x))} ))
annotated_X.clr = bind_cols(separated_X[1:3], as.tibble(X.df.clr))
```

## PCA

We can attempt PCA on the transformed data. We scale to unit variance.

```{r messages=FALSE}
library(ggfortify)
library(nFactors)
library(GGally)
X.df.clr.PCA1 = prcomp(X.df.clr, scale=T)
autoplot(X.df.clr.PCA1, data=annotated_X.clr, colour="Status", x=1, y=2)
```

We can see that the first component accounts for a lot of variance. It would be informative to attempt parallel analysis to determine the number of principal components to retain. This technique is a formalisation of the Scree plot and works by simulating a random data frame with the same number of of samples and variables as in the original data frame, computing a correlation matrix for this random d.f. We use the eigenvalues from this simulation to decide if the components in our real analysis are likely to be random noise.

```{r}
parallelAnal1 = parallel(subject=nrow(X.df.clr), var=ncol(X.df.clr), rep=100)
screeTest1 = nScree(x=eigen(cor(X.df.clr))$values, aparallel=parallelAnal1$eigen$qevpea)
plotnScree(screeTest1)
```

This analysis suggests around 16 components would capture most of the non-random variability. We can plot at least the first few components against each other more or less coherently.

```{r}
PCA1_and_desc = bind_cols(as_tibble(X.df.clr.PCA1$x), separated_X[1:3])
ggpairs(PCA1_and_desc, aes(colour=Status), columns=1:7, progress = FALSE)
```

```{r}
ggpairs(PCA1_and_desc, aes(colour=Dataset), columns=1:7, progress = FALSE)
```

# Non-Metric Multidimensional Scaling



# tSNE

t-Distributed Stochastic Neighbor Embedding is a non-linear dimensionality reduction method. We embed the data into two dimensions.

```{r}
library(Rtsne)
tsne1 = Rtsne(X, dims=2, perplexity=30, max_iter=400)
tSNE1_res = as_tibble(tsne1$Y)
tsne_with_additionals = bind_cols(tSNE1_res, separated_X[1:3])
ggplot(tsne_with_additionals, aes(x=V1, y=V2,colour=Dataset)) + geom_point() 
```

```{r}
ggplot(tsne_with_additionals, aes(x=V1, y=V2,colour=Status)) + geom_point() 
```

# Constrained ordination

It is clear from our previous attempts that running unconstrained ordination on this dataset merely reveals considerable bias in collection methods accross different country sites, with also perhaps some country-specific structure to the microflora of the patient guts. We are really, however, interested in seeing the clinical structure in this data. Thus we have to substract or account for the collection and geographic bias. We do this using constrained ordination, conditioning on the Dataset variable.

```{r}
library(vegan)
constrainedModel1 = capscale(separated_X[, -c(1,2,3)] ~ Status + Condition(Dataset), data=separated_X)
constrainedModel1Summary = summary(constrainedModel1)
ggpairs(as.tibble(constrainedModel1Summary$species), progress = FALSE)
```


```{r}
constrainedModel2 = cca(separated_X[, -c(1,2,3)] ~ Status + Condition(Dataset), data=separated_X)
constrainedModel2Summary = summary(constrainedModel2)
ggpairs(as.tibble(constrainedModel2Summary$species), progress = FALSE)
```

# Bibliography