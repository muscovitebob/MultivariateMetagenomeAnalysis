---
title: "Multivariate statistical analysis of metagenomic data"
author: "Boris Shilov"
bibliography: "bibliography.bib"
output:
  pdf_document: default
---

# Introduction

The aim of this analysis is to explore the metagenomic dataset assembled by @Forslund:2015aa.

## Data format

```{r warning=FALSE, message=FALSE}
library(tidyverse)
set.seed(100)
data = read_tsv("data.r")
data
```

We can see that the dataset consists of six columns. The first is sample IDs, which are not unique keys since every sample may have multiple features. The second is the dataset of origin, which is either Danish (MHD), Swedish (SWE) or Han Chinese (CHN). The third column is treatment status of a particular patient, where they are either healthy (ND CTRL), are having type 2 diabetes treated with metformin included (T2D metformin+), are having type 2 diabetes treated without metformin (T2D metformin-) or are having type 1 diabetes treated (T1D). The fourth column specifies the type of feature in the fifth column. This is either gut microbial/metabolic module (GMM), SEED database annotation, bacterial family, bacterial genus or metagenomic operational taxonomic units (Motu). The fifth column contains the feature names which are coded differently depending on the feature type. Finally, the sixth column contains the abundance of a given feature.

The feature types are split into two spaces: the taxonomic space represented by the genuses, families and taxonomic units, and the functional space as annotated by SEED and GMM. The full SEED and GMM feature code annotations can be found in the appendix files, taken from Supplementary Table 10 of @Forslund:2015aa. We are only interested in exploring the structure of the functional space.

```{r}
functional_space = filter(data, FeatureType %in% c("GMM", "SEED"))
functional_space
```

In order to actually perform analysis, we transform the data into the data matrix $X$, with every row being a sample and every column a particular feature. First we combine redundant columns.

```{r}
united_functional_space = functional_space %>%
  unite(SampleInfo, Sample, Dataset, Status, sep="~") %>% 
  unite(FeatureInfo, FeatureType, Feature, sep="~")
```

```{r}
X = united_functional_space %>% group_by(FeatureInfo) %>% spread(FeatureInfo, Abundance)
```

There is significant missingness in 25 of the individuals, and we exclude them from further analysis.

```{r}
X = drop_na(X)
X
```

# Analysis

## PCA

```{r}
train = X %>% sample_frac(0.8)
test = anti_join(X, train, by="SampleInfo")
```


```{r warning=FALSE, messages=FALSE}
library(nFactors)
eigenvals = eigen(cor(X[-1]))
eigenvaldist = parallel(subject=nrow(X[-1]), var=ncol(X[-1]), rep=100)
nS = nScree(x=eigenvals$values, aparallel=eigenvals$eigen$qevpea)
plotnScree(nS)
```

```{r}
cov1 = cov(train[-1])
cor1 = cov2cor(cov1)
eigens = eigen(cov1)
PCA1 = prcomp(train[-1], scale=F, center=T)
pairs(PCA1$x[,1:5])
```

```{r}
PCA1_parallel = parallel(subject=nrow(train[-1]), var=ncol(train[-1]), rep=100)
PCA1_scree = nScree(x=PCA1$sdev^2, aparallel=PCA1_parallel$eigen$qevpea)
plotnScree(PCA1_scree)
```

This suggests it would be optimal to retain in the hundreds of components to explain most of the variance. Instead, we can use the Kaiser-Guttman rule to select the first 40 or so components, which explain about 93% - a large reduction in factors in exchange for a small reduction in the explained variance.

```{r}
PCA2 = prcomp(train[-1], scale=F, center=T, tol=0.07)
train.df = as.data.frame(train[,-1])
row.names(train.df) = pull(train, SampleInfo)
model1scores = as.matrix(train[-1]) %*% as.matrix(PCA2$rotation)
prediction1 = predict(PCA2, test[-1])
```

# tSNE

```{r}
library(Rtsne)
tsne1 = Rtsne(train[,-1], dims=2, perplexity=30, max_iter=400)
tSNE1_res = as_tibble(tsne1$Y)
separated = train[,1] %>% separate(SampleInfo, into=c("Sample", "Dataset", "Status"), sep="~")
status_vector = separated["Status"]
tsne_with_status = bind_cols(tSNE1_res, status_vector)
ggplot(tsne_with_status, aes(x=V1, y=V2,colour=Status)) + geom_point() 
```


# Bibliography